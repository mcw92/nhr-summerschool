{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHR Summer School â€“ Data-Parallel Neural Networks with `PyTorch`\n",
    "##### Dr. Charlotte Debus (charlotte.debus@kit.edu), Dr. Marie Weiel (marie.weiel@kit.edu), and David Li (david.li@kit.edu)\n",
    "#### Agenda\n",
    "\n",
    "| W H E N           | W H A T                                                 |\n",
    "| :-----------------| :------------------------------------------------------ |\n",
    "| **09:00 - 10:15** | **Introduction to Neural Networks**                     |  \n",
    "|                   | Backpropagation and Stochastic Gradient Descent (SGD)   |  \n",
    "|                   | Layer Architectures                                     |  \n",
    "|                   | Training a Neural Network                               |  \n",
    "| 10:15 - 10:30     | *It's coffee o'clock!*                                  |\n",
    "| **10:30 - 12:00** | **Hands-on Session: Neural Networks with `PyTorch`**    |  \n",
    "| 12:00 - 13:00     | *Enjoy your lunch break!*                               |  \n",
    "| **13:00 - 14:15** | **Data-Parallel Neural Networks**                       |  \n",
    "|                   | Parallelization Strategies for Neural Networks          |  \n",
    "|                   | Distributed SGD                                         |  \n",
    "|                   | IID and Large Minibatch Effects                         |  \n",
    "|   14:15 - 14:30   | *It's coffee o'clock!*                                  |\n",
    "| **<font color='orange'>14:30 - 16:00</font>** | **<font color='orange'>Hands-on Session:</font> `PyTorch DistributedDataParallel`** |\n",
    "\n",
    "\n",
    "## Hands-on Session: `PyTorch DistributedDataParallel`\n",
    "Today in the morning, you learned how to train a neural network in `PyTorch` using the example of [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) classification with the convolutional neural network *AlexNet*. In this hands-on tutorial, you will learn how to train the same network in a distributed data-parallel fashion. We will use `PyTorch`'s `DistributedDataParallel` module for this. \n",
    "\n",
    "### Short recap\n",
    "#### AlexNet\n",
    "\n",
    "*AlexNet* is a CNN for image classification, originally of the [ImageNet](https://www.image-net.org/) dataset. \n",
    "The input is an RGB image and the output is a vector of $n_\\text{classes}$ numbers that sum up to 1, where the $i^\\text{th}$ element can be interpreted as the probability that the input image belongs to class $i$. \n",
    "*AlexNet* consists of five convolutional layers, some followed by max-pooling, and three fully-connected layers. It uses the non-saturating ReLU activation function.  \n",
    "\n",
    "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. **[Imagenet classification with deep convolutional neural networks.](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)** *Advances in Neural Information Processing Systems* 25 (2012): 1097-1105.   \n",
    "\n",
    "![AlexNet architecture.](alexnet_torch.png \"Architecture of AlexNet: AlexNet consists of eight layers: the first five are convolutional layers, some followed by max-pooling layers, the last three are fully connected layers. It uses the non-saturating ReLU activation function.\")  \n",
    "\n",
    "#### CIFAR-10\n",
    "The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset contains 60,000 color images of size 32 x 32 from ten classes, where each class holds 6000 images. \n",
    "The dataset is divided into five training batches and one test batch, each containing 10,000 images. \n",
    "The test batch contains exactly 1000 randomly selected images from each class. \n",
    "The training batches contain the remaining images in random order. \n",
    "Below you see the classes of the dataset and ten random images from each class:  \n",
    "![CIFAR-10-Dataset.](Cifar.png \" \")  \n",
    "\n",
    "Source: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "#### Data-parallel neural networks (DPNNs) in `PyTorch`\n",
    "\n",
    "After lunch, you learned about data-parallel training of neural networks. \n",
    "As you already know, this involves distributing the training process across multiple processors to accelerate computation and increase training throughput. \n",
    "`PyTorch` provides the `DistributedDataParallel` (`DDP`) module for this, which abstracts away some of the complexities of implementing data-parallel training in a distributed setting. \n",
    "From the official [documentation](https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html):  \n",
    ">Distributed data-parallel training is a widely adopted single-program multiple-data training paradigm. The model is replicated on every process, and every model replica will be fed with a different set of input data samples. The `DistributedDataParallel` module takes care of gradient communication to keep model replicas synchronized and overlaps it with the gradient computations to speed up training. \n",
    "It implements data parallelism at the module level which can run across multiple machines. Applications using `DDP` should spawn multiple processes and create a single `DDP` instance per process. `DDP` uses collective communications in the `torch.distributed` package to synchronize gradients and buffers. More specifically, `DDP` registers an autograd hook for each parameter given by `model.parameters()` and the hook will fire when the corresponding gradient is computed in the backward pass. Then `DDP` uses that signal to trigger gradient synchronization across processes.  \n",
    "*The recommended way to use `DDP` is to spawn one process for each model replica. `DDP` processes can be placed on the same machine or across machines, but GPU devices cannot be shared across processes.*  \n",
    "\n",
    "The `torch.distributed` package supports three built-in backends for communication between processors. \n",
    "This [table](https://pytorch.org/docs/stable/distributed.html#backends) shows which functions are available for use with CPU/CUDA tensors. \n",
    "Since *Noctua2* connects GPUs with NVLink within a node and Mellanox Infiniband Interconnect between nodes, we use the officially recommended NCCL backend. \n",
    "The [NVIDIA Collective Communication Library](https://developer.nvidia.com/nccl) (NCCL) implements multi-GPU and multi-node communication functions optimized for NVIDIA GPUs and networks. \n",
    "It provides routines, such as all-gather, all-reduce, broadcast, reduce, reduce-scatter, and point-to-point transmit and receive. \n",
    "\n",
    "#### How to train a DPNN with `PyTorch`'s `DistributedDataParallel` module\n",
    "Below is a recipe for training a DPNN with `DDP` in `PyTorch`:\n",
    "\n",
    "1. **Initialize the distributed environment:** Before using `DDP`, you need to define and initialize the distributed environment. This involves setting up the communication backend (NCCL for us), specifying the so-called process group, and assigning a unique rank and the world size to each process in the process group. The rank is like a unique process ID and the world size corresponds to the overall number of processes you want to use. \n",
    "2. **Load the data:** Data parallelism means splitting the input data across the processes in the process group and computing the forward and backward passes independently on each rank. This enables parallel processing and reduces the training time. You load the training and validation datasets and distribute them equally over the processes so that each process holds a different, exclusive subset of each dataset. `PyTorch` provides a dedicated sampler for this, the so-called `DistributedSampler`.\n",
    "3. **Model instantiation and replication:** Afterwards, you need to replicate the model across the processes. Each replica will process a subset of the input data provided by the `DistributedSampler`. To do so, you instantiate the model just as in the serial case and wrap it with `DDP`. This ensures that the gradients computed during the backward pass are synchronized across all replicas.\n",
    "4. **Training loop:** Repeat for a specified number of iterations or until convergence is reached:\n",
    "    - *Forward pass*: Each replica of the model independently processes its portion of the input data. \n",
    "    - *Backward pass and gradient synchronization*: The gradients are computed independently on each replica. They are then synchronized across all replicas using a function called \"all-reduce\". This step ensures that the model parameters are updated consistently across all processes.\n",
    "    - *Optimization step:* Once the gradients are synchronized, the optimizer performs an optimization step to update the model parameters. This step is performed independently and redundantly on each replica.\n",
    "    - *Validation*: After updating the model parameters, you can compute the current model's accuracy on the training and validation dataset. As each process only holds a portion of each dataset, you need to implement some more communication to obtain the accuracy on each whole dataset. \n",
    "5. **Evaluation:** After training, you can evaluate the final model's performance using a held-out test dataset. The evaluation is typically performed on a single process without the need for data parallelism. \n",
    "\n",
    "### What you will do now\n",
    "\n",
    "Building on our morning hands-on session, you will learn how to train a data-parallel version of *AlexNet* in `PyTorch`. This tutorial will guide you through the steps required for parallelizing the training in a data-parallel fashion using `DDP`. \n",
    "As parallel runs are inconvenient using Jupyter Notebooks, you will need to create `Python` scripts from the code snippets provided in this notebook and run the scripts as a batch job on *Noctua2*.\n",
    "\n",
    "The tutorial is structured as follows:\n",
    "\n",
    "-------------\n",
    "1. Get all the building blocks.\n",
    "- **Model:** Define your model <font color='grey'> (~0 min)</font>.  \n",
    "- **Data:** Define the dataloaders <font color='grey'> (~20 min)</font>.   \n",
    "\n",
    "*Short break with everyone to discuss your results and possible solutions.*  \n",
    "\n",
    "- **Training:** Define the training loop <font color='grey'> (~20 min)</font>.\n",
    "\n",
    "*Short break with everyone to discuss your results and possible solutions.*  \n",
    "\n",
    "2. Assemble the main `Python` script from your building blocks <font color='grey'> (~20 min)</font>. \n",
    "\n",
    "*Short break with everyone to discuss your results and possible solutions.*  \n",
    "\n",
    "3. Run your code in parallel as a batch job on *Noctua2* <font color='grey'> (~30 min)</font>. \n",
    "- Use four GPUs, i.e., four processes in the data-parallel training process. \n",
    "- Start your `Python` script in parallel with the `srun` ([doc](https://slurm.schedmd.com/srun.html)) command in a job bash script. \n",
    "- Submit your job script to the [SLURM](https://www.schedmd.com/) workload manager with `sbatch` ([doc](https://slurm.schedmd.com/sbatch.html)).\n",
    "\n",
    "*Final break with everyone to discuss your results and possible solutions.*  \n",
    "\n",
    "-------------\n",
    "\n",
    "Below is a corresponding code framework with dataloaders for the training including validation and testing, along with detailed explanations and instructions for each step. \n",
    "**Normal comments with '#' describe code as usual, in lines with '##' you need to add code.** \n",
    "\n",
    "## 1. Get all the building blocks\n",
    "\n",
    "### Model: Define the model\n",
    "As the first step, you again need to define your model architecture. \n",
    "This is just a copy-paste of your `AlexNet` module class from the serial case. \n",
    "Later on, you will wrap an instance of this module with `DDP` to make it distributed data-parallel (the magic happens here). \n",
    "Save the code below as a separate `Python` module file `model.py` so that you can import the `AlexNet` module class from this file into your main script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# Define neural network by subclassing PyTorch's nn.Module. \n",
    "class AlexNet(torch.nn.Module):\n",
    "    \n",
    "    # Initialize neural network layers in __init__. \n",
    "    def __init__(self, num_classes = 1000, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.features = torch.nn.Sequential(\n",
    "            # AlexNet has 8 layers: 5 convolutional layers, some followed by max-pooling (see figure),\n",
    "            # and 3 fully connected layers. In this model, we use nn.ReLU between our layers, \n",
    "            # but there are other activations to introduce non-linearity in a model.\n",
    "            # nn.Sequential is an ordered container of modules. \n",
    "            # The data is passed through all the modules in the same order as defined. \n",
    "            # You can use sequential containers to put together a quick network.\n",
    "            #\n",
    "            # IMPLEMENT FEATURE-EXTRACTOR PART OF ALEXNET HERE!\n",
    "            # 1st convolutional layer (+ max-pooling)\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # 2nd convolutional layer (+ max-pooling)\n",
    "            torch.nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # 3rd + 4th convolutional layer\n",
    "            torch.nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # 5th convolutional layer (+ max-pooling)\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        # Average pooling to downscale possibly larger input images.\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = torch.nn.Sequential( \n",
    "            # IMPLEMENT FULLY CONNECTED MULTI-LAYER PERCEPTRON PART HERE!\n",
    "            # 6th, 7th + 8th fully connected layer \n",
    "            # The linear layer is a module that applies a linear transformation \n",
    "            # on the input using its stored weights and biases.\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(256 * 6 * 6, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    # Forward pass: Implement operations on the input data, i.e., apply model to input x.\n",
    "    def forward(self, x):\n",
    "        # IMPLEMENT OPERATIONS ON INPUT DATA x HERE!\n",
    "        x = self.features(x)    # Apply feature-extractor part to input.\n",
    "        x = self.avgpool(x)     # Apply average-pooling part.\n",
    "        x = torch.flatten(x, 1) # Flatten.\n",
    "        x = self.classifier(x)  # Apply fully connected multilayer perceptron part.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: Define dataloaders\n",
    "Next, you again need to get the data in. You already learned that to train a DPNN, each process needs to load an exclusive subset of the dataset. \n",
    "`PyTorch` provides a dedicated sampler to distribute and load data in a distributed training setting, the so-called `DistributedSampler`.\n",
    "It enables efficient data loading across multiple processes by partitioning the dataset into smaller subsets that are processed independently by each process.\n",
    "The `DistributedSampler` works in conjunction with `DDP`. It ensures that each process operates on a unique subset of the dataset, avoiding redundant computation and enabling parallelism. \n",
    "Below, you can find an overview of how this works:\n",
    "\n",
    "1. **Data partitioning:** The `DistributedSampler` partitions the dataset into smaller subsets based on the number of processes involved in the distributed training. Each process is responsible for processing a specific subset of the data.\n",
    "\n",
    "2. **Shuffling and sampling:** Optionally, the `DistributedSampler` can shuffle the dataset before partitioning it to introduce randomness into the training. This helps prevent biases and improves the model's generalization. The shuffling is typically performed on a single process, and the shuffled indices are then broadcasted to other processes.\n",
    "\n",
    "3. **Data loading:** During training, each process loads its assigned subset of the dataset using the `DistributedSampler`. The sampler provides indices corresponding to the samples in the process's partition of the dataset.\n",
    "\n",
    "4. **Parallel processing:** Once the data is loaded, each process operates independently on its portion of the dataset. Forward and backward passes, as well as the optimization step, are performed separately on each process.\n",
    "\n",
    "5. **Synchronization:** After each training iteration, the processes synchronize to ensure that the model parameters and gradients are consistent across all processes. This synchronization is handled by `DDP`.\n",
    "\n",
    "6. **Iteration and epoch completion:** The `DistributedSampler` manages the completion of iterations and epochs. It ensures that each process finishes processing its assigned subset of the data before moving on to the next iteration or epoch. The `DistributedSampler` may also reshuffle the dataset at the end of each epoch to introduce further randomness.\n",
    "\n",
    "Complete the code below and save it as a separate `Python` module file `helper_dataset.py` so that you can import the dataloader from this file into your main script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "def get_dataloaders_cifar10_ddp(\n",
    "    batch_size, \n",
    "    num_workers=0,\n",
    "    root='data',\n",
    "    validation_fraction=0.1,\n",
    "    train_transforms=None,\n",
    "    test_transforms=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Get distributed CIFAR10 dataloaders for training and validation in a DDP setting.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    batch_size : int\n",
    "                 batch size\n",
    "    num_workers : int\n",
    "                  How many workers to use for data loading.\n",
    "    root : str\n",
    "           path to data dir\n",
    "    validation_fraction : float\n",
    "                          fraction of train dataset used for validation\n",
    "    train_transforms : torchvision.transforms.<transformation>\n",
    "                       How to preprocess the training data.\n",
    "    test_transforms : torchvision.transforms.<transformation>\n",
    "                      How to preprocess the test data.\n",
    "                      \n",
    "    Returns\n",
    "    -------\n",
    "    torch.utils.data.Dataloader : training dataloader\n",
    "    torch.utils.data.Dataloader : validation dataloader\n",
    "    \"\"\"\n",
    "    if train_transforms is None: \n",
    "        train_transforms = torchvision.transforms.ToTensor()\n",
    "    if test_transforms is None: \n",
    "        test_transforms = torchvision.transforms.ToTensor()\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        transform=train_transforms,\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    valid_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        transform=test_transforms\n",
    "    )\n",
    "\n",
    "    # Perform index-based train-validation split of original training data. \n",
    "    ## total = ... # Get overall number of samples in original training data.\n",
    "    ## idx = ...   # Make an index list for your training samples.\n",
    "    ## Shuffle indices.\n",
    "    ## vnum = ...  # Determine number of validation samples from validation split.\n",
    "    ## train_indices, valid_indices = ... # Extract train and validation indices from shuffled index list.\n",
    "\n",
    "    # Split into training and validation dataset according to specified validation fraction.\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    valid_dataset = torch.utils.data.Subset(valid_dataset, valid_indices)\n",
    "\n",
    "    # Sampler that restricts data loading to a subset of the dataset.\n",
    "    # Especially useful in conjunction with DistributedDataParallel. \n",
    "    # Each process can pass a DistributedSampler instance as a DataLoader sampler, \n",
    "    # and load a subset of the original dataset that is exclusive to it.\n",
    "\n",
    "    # Get samplers.\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=torch.distributed.get_world_size(),\n",
    "        rank=torch.distributed.get_rank(),\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        valid_dataset,\n",
    "        num_replicas=torch.distributed.get_world_size(),\n",
    "        rank=torch.distributed.get_rank(),\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # Get dataloaders.\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "        sampler=valid_sampler\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Short break with everyone to discuss your results and possible solutions.*\n",
    "\n",
    "### Training: Define the training loop\n",
    "Now that we have our `AlexNet` model and the distributed CIFAR-10 data, we want to actually train, validate, and test it by optimizing its parameters in a data-parallel fashion with `DDP`. \n",
    "Remember that training a model is an iterative process. In each iteration, the model predicts the output for a given input, calculates the error in its prediction as quantified by the loss function, collects the derivatives of the loss w.r.t. its parameters, and optimizes these parameters using gradient descent. \n",
    "As `DDP` handles the synchronization of the gradients over all processes for you, the structure of the training loop stays basically the same as before. \n",
    "While each processor trains its model replica on its local training data batch provided by the `DistributedSampler`, \n",
    "`DDP` takes care of gradient communication to keep the model replicas synchronized. \n",
    "You might also want to track the average loss over all processes during the training. \n",
    "As `DDP` only takes care of the gradient synchronization, you have to implement this explicitly using collective communication functions from `torch.distributed` ([doc](https://pytorch.org/docs/stable/distributed.html#collective-functions)).  \n",
    "\n",
    "Similar to the serial case, we will define some useful helper functions for validating our model during training and testing it afterwards on unseen data:\n",
    "- `get_right_ddp`: Get the number of correctly predicted and overall samples for a given model on a given dataset. You will need those numbers for calculating your model's accuracy during the training loop on the distributed training and validation datasets.\n",
    "- `compute_accuracy_ddp`: Compute the accuracy of your model's predictions on a given dataset. You will need this function for testing your final model on a held-out test dataset after the training is done. Conceptually the same as for the serial case with some slight technical differences.\n",
    "\n",
    "All of this functionality is defined in the `train_model_ddp` function below. \n",
    "Complete the code and save it as a separate `Python` module file `helper_train.py` so that you can import the training function from this file into your main script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_accuracy_ddp(model, data_loader):\n",
    "    \"\"\"\n",
    "    Compute accuracy of model predictions on given labeled data.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model : torch.nn.Module\n",
    "            Model.\n",
    "    data_loader : torch.utils.data.Dataloader\n",
    "                  Dataloader.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float : The model's accuracy on the given dataset in percent.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.cuda()\n",
    "            targets = targets.float().cuda()\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1) # Get class with highest score.\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float() / num_examples * 100\n",
    "\n",
    "def get_right_ddp(model, data_loader):\n",
    "    \"\"\"\n",
    "    Compute the number of correctly predicted samples and the overall number of samples in a given dataset.\n",
    "    \n",
    "    This function is needed to compute the accuracy over multiple processors in a distributed data-parallel setting.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model : torch.nn.Module\n",
    "            Model.\n",
    "    data_loader : torch.utils.data.Dataloader\n",
    "                  Dataloader.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int : The number of correctly predicted samples.\n",
    "    int : The overall number of samples in the dataset.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.cuda()\n",
    "            targets = targets.float().cuda()\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1) # Get class with highest score.\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    num_examples = torch.Tensor([num_examples]).cuda()\n",
    "    return correct_pred, num_examples\n",
    "\n",
    "def train_model_ddp(\n",
    "    model,\n",
    "    num_epochs,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Train model in distributed data-parallel fashion.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    model : torch.nn.Module\n",
    "            model to train\n",
    "    num_epochs : int\n",
    "                 number of epochs to train\n",
    "    train_loader : torch.utils.data.Dataloader\n",
    "                   training dataloader\n",
    "    valid_loader : torch.utils.data.Dataloader\n",
    "                   validation dataloader\n",
    "    optimizer : torch.optim.Optimizer\n",
    "                optimizer to use\n",
    "    \"\"\"\n",
    "    ## start = ... # Start timer to measure training time.    \n",
    "    rank = torch.distributed.get_rank() # Get local process ID (= rank).\n",
    "    world_size = torch.distributed.get_world_size() # Get overall number of processes.\n",
    "\n",
    "    loss_history, train_acc_history, valid_acc_history = [], [], [] # Initialize history lists.\n",
    "\n",
    "    # Actual training starts here.\n",
    "    for epoch in range(num_epochs): # Loop over epochs.\n",
    "\n",
    "        train_loader.sampler.set_epoch(epoch) # Set current epoch for distributed dataloader.\n",
    "        \n",
    "        ## Set model to training mode.\n",
    "\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader): # Loop over mini batches.\n",
    "\n",
    "            # Convert dataset to GPU device.\n",
    "            features = features.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            # FORWARD & BACKWARD PASS (also see first hands-on session)\n",
    "            ## logits = ... # Get predictions of current model from forward pass.\n",
    "            ## loss = ...   # Use cross-entropy loss.\n",
    "            ## Zero out gradients (by default, gradients are accumulated in buffers in backward pass).\n",
    "            ## Backward pass.\n",
    "            ## Update model parameters in single optimization step.\n",
    "            #\n",
    "            # LOGGING\n",
    "            ## Calculate effective mini-batch loss as process-averaged mini-mini-batch loss.\n",
    "            ## Sum up mini-mini-batch losses from all processes and divide by number of processes.\n",
    "            ## Use collective communication functions from torch.distributed package.\n",
    "            # Note that torch.distributed collective communication functions will only\n",
    "            # work with torch tensors, i.e., floats, ints, etc. must be converted before!\n",
    "            ## Append globally averaged loss of this epoch to history list.\n",
    "\n",
    "            if rank == 0:\n",
    "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
    "                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n",
    "                      f'| Averaged Loss: {loss:.4f}')\n",
    "\n",
    "        # Validation starts here.\n",
    "        \n",
    "        ## Set model to evaluation mode.\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculation.\n",
    "            # Validate model in data-parallel fashion.\n",
    "            # Determine number of correctly classified samples and overall number \n",
    "            # of samples in training and validation dataset.\n",
    "            #\n",
    "            ## right_train, num_train = get_right_ddp(...)\n",
    "            ## right_valid, num_valid = get_right_ddp(...)\n",
    "            #\n",
    "            ## Sum up number of correctly classified samples in training dataset,\n",
    "            ## overall number of considered samples in training dataset, \n",
    "            ## number of correctly classified samples in validation dataset,\n",
    "            ## and overall number of samples in validation dataset over all processes.\n",
    "            ## Use collective communication functions from torch.distributed package.\n",
    "            #\n",
    "            # Note that torch.distributed collective communication functions will only\n",
    "            # work with torch tensors, i.e., floats, ints, etc. must be converted before!\n",
    "            # From these values, calculate overall training + validation accuracy.\n",
    "            #\n",
    "            ## train_acc = ...\n",
    "            ## valid_acc = ...\n",
    "            ## Append accuracy values to corresponding history lists.\n",
    "\n",
    "            if rank == 0:\n",
    "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
    "                  f'| Train: {train_acc :.2f}% '\n",
    "                  f'| Validation: {valid_acc :.2f}%')\n",
    "\n",
    "        elapsed = (time.perf_counter() - start) / 60 # Measure training time per epoch.\n",
    "        Elapsed = torch.Tensor([elapsed]).cuda()\n",
    "        torch.distributed.all_reduce(Elapsed)\n",
    "        Elapsed /= world_size\n",
    "        if rank == 0:\n",
    "            print(f'Time elapsed: {Elapsed.item()} min')\n",
    "\n",
    "    ## elapsed = ... # Stop timer and calculate training time elapsed after epoch.    \n",
    "    Elapsed = torch.Tensor([elapsed]).cuda()\n",
    "    ## Calculate average training time elapsed after each epoch over all processes,\n",
    "    ## i.e., sum up times from all processes and divide by overall number of processes.\n",
    "    ## Use collective communication functions from torch.distributed package.\n",
    "    # Note that torch.distributed collective communication functions will only\n",
    "    # work with torch tensors, i.e., floats, ints, etc. must be converted before!\n",
    "        \n",
    "    if rank == 0:\n",
    "        ## Print process-averaged training time after each epoch.        \n",
    "        torch.save(loss_history, f'loss_{world_size}_gpu.pt')\n",
    "        torch.save(train_acc_history, f'train_acc_{world_size}_gpu.pt')\n",
    "        torch.save(valid_acc_history, f'valid_acc_{world_size}_gpu.pt')\n",
    "\n",
    "    return loss_history, train_acc_history, valid_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Short break with everyone to discuss your results and possible solutions.*\n",
    "\n",
    "## 2. Assemble the main `Python` script from your building blocks\n",
    "Now that you have implemented all the functions and classes you need, you are ready to put together the main `Python` script that is to be executed in parallel on the supercomputer. \n",
    "As explained above, you need to set up the so-called process group first. \n",
    "After this has been done properly, you can load your data so that each process holds an exclusive subset, instantiate your module, wrap it with `DDP`, and train it in a data-parallel fashion on the process-local data. \n",
    "As `DDP` broadcasts model states from the process with rank 0 (often called root) to all other processes in the `DDP` constructor, you do not need to worry about different `DDP` processes starting from different initial model parameter values. \n",
    "As you have seen, `DDP` wraps lower-level distributed communication details and provides a clean API as if it were a local model. \n",
    "Gradient synchronization communications take place during the backward pass and overlap with the backward computation. \n",
    "When the `backward()` returns, `param.grad` already contains the synchronized gradient tensor. \n",
    "\n",
    "Complete the code below and save it as a separate `Python` script `main.py` in the same folder as all your helper module files. This file is the one to be actually run in parallel on *Noctua2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from model import AlexNet\n",
    "from helper_dataset import get_dataloaders_cifar10_ddp\n",
    "from helper_train import train_model_ddp, get_right_ddp, compute_accuracy_ddp\n",
    "\n",
    "def main():\n",
    "    \n",
    "    ## world_size = int(os.getenv(\"...\") # Get overall number of processes from SLURM environment variable.\n",
    "    ## rank = int(os.getenv(\"...\")       # Get individual process ID from SLURM environment variable.\n",
    "    slurm_job_gpus = os.getenv(\"SLURM_JOB_GPUS\")\n",
    "    slurm_localid = int(os.getenv(\"SLURM_LOCALID\"))\n",
    "    gpus_per_node = torch.cuda.device_count()\n",
    "    gpu = rank % gpus_per_node\n",
    "    assert gpu == slurm_localid\n",
    "    device = f\"cuda:{slurm_localid}\"\n",
    "    ## Set device. \n",
    "\n",
    "    # Initialize DDP.\n",
    "    ## torch.distributed.init_process_group(...)\n",
    "    ## Check if process group has been initialized successfully.\n",
    "    ## Check used backend.\n",
    "    \n",
    "    b = 256 # Set batch size.\n",
    "    e = 100 # Set number of epochs to be trained.\n",
    "\n",
    "    # Define transforms for data preprocessing to make smaller CIFAR-10 images work with AlexNet.\n",
    "    # You can find a more detailed explanation in the notebook of the first hands-on session.\n",
    "    train_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((70, 70)),\n",
    "        torchvision.transforms.RandomCrop((64, 64)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    test_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((70, 70)),\n",
    "        torchvision.transforms.CenterCrop((64, 64)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Get distributed dataloaders for training and validation data on all ranks.\n",
    "    ## train_loader, valid_loader = get_dataloaders_cifar10_ddp(...)\n",
    "    # Data is here: scratch/hpc-prf-nhrgs/mweiel/data\n",
    "\n",
    "    # Get dataloader for test data. \n",
    "    # Final testing is only done on root.\n",
    "    if dist.get_rank() == 0:\n",
    "        test_dataset = torchvision.datasets.CIFAR10(\n",
    "            root=\"/scratch/hpc-prf-nhrgs/mweiel/data\",\n",
    "            train=False,\n",
    "            transform=test_transforms\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=b,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    ## model = ...     # Create AlexNet model with 10 classes for CIFAR-10 and move it to GPU.\n",
    "    ## ddp_model = ... # Wrap model with DDP.\n",
    "    \n",
    "    # Set up stochastic gradient descent optimizer from torch.optim package.\n",
    "    # Use a momentum of 0.9 and a learning rate of 0.1.\n",
    "    # Use parameters of DDP model here!\n",
    "    ## optimizer = ... \n",
    "    \n",
    "    # Train DDP model.\n",
    "    ## loss_history, train_acc_history, valid_acc_history = train_model_ddp(...)\n",
    "    \n",
    "    # Test final model on root.\n",
    "    if dist.get_rank() == 0:\n",
    "        ## test_acc = compute_accuracy_ddp(...) # Compute accuracy on test data.\n",
    "        ## Print test accuracy. \n",
    "        \n",
    "    ## Destroy process group. \n",
    "\n",
    "# MAIN STARTS HERE.    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Short break with everyone to discuss your results and possible solutions.*\n",
    "\n",
    "## 3. Run your code in parallel as batch job on *Noctua2* \n",
    "Now it's time to actually run your script on *Noctua2*. \n",
    "As a normal user, you do not have root rights on a supercomputer. This means you cannot install any software you want but software is made available to you via pre-installed modules. On top of that, you cannot use compute resources as you wish but compute resources are managed among all users by a so-called job scheduler. \n",
    "\n",
    "Job scheduling on a supercomputer is like managing a busy restaurant. Imagine you have a restaurant with many tables, and many customers want to be served at the same time. To serve everyone efficiently, you need a system to manage who gets seated at which table and when. Similarly, many people or organizations typically want to use a supercomputer to run their programs or simulations at the same time. Job scheduling is the process of deciding which jobs (programs or tasks) should run on the supercomputer, when they should start, and how long they can use the resources. \n",
    "The job scheduler, like a restaurant manager, looks at the incoming jobs and determines the best order and timing for execution. It takes into account factors such as job priority, estimated runtime, resource availability, and fairness. For example, a critical job that requires a lot of computing power may be given a higher priority and scheduled to run as soon as possible, while a smaller job that can be completed quickly might be scheduled to run in between larger jobs. \n",
    "The job scheduler also ensures that the supercomputer's resources, like processors, memory, and storage, are used efficiently. It assigns these resources to different jobs based on their requirements and availability, making sure that multiple jobs can run concurrently without interfering with each other. In this way, a job scheduler on a supercomputer manages the incoming workload, organizes the jobs, and allocates resources effectively to maximize the utilization and performance of the supercomputer, just like a restaurant manager aims to serve all customers in the most efficient way possible.\n",
    "\n",
    "As many of the world's supercomputers and computer clusters, *Noctua2* uses the SLURM workload manager, a free and open-source job scheduler for Linux kernels. \n",
    "It provides three key functions:\n",
    "- Allocating access to compute resources to users for some time so they can perform work,\n",
    "- Providing a framework for starting, executing, and monitoring work, and\n",
    "- Arbitrating contention for resources by managing a queue of pending jobs.\n",
    "\n",
    "SLURM is the workload manager on about 60\\% of the TOP500 supercomputers.\n",
    "To run a job on a supercomputer, you need to submit a batch job script to SLURM with the `sbatch` ([doc](https://slurm.schedmd.com/sbatch.html)) command. \n",
    "This job script specifies which compute resources you need for how long, along with the actual code to run. \n",
    "Below you find a job script requesting four GPUs on one node. \n",
    "Use the `srun` ([doc](https://slurm.schedmd.com/srun.html)) command to execute your `Python` script in parallel on the requested four GPUs.\n",
    "Adapt the code for your needs and save it as a separate bash script `submit_4_gpu.sh`.  \n",
    "To run your script on *Noctua2*, submit it to the SLURM workload manager: `sbatch submit_4_gpu.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=alex4\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --gres=gpu:a100:4\n",
    "#SBATCH --time=30:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --account=hpc-prf-nhrgs\n",
    "#SBATCH --ntasks-per-node=4\n",
    "#SBATCH --output=/scratch/hpc-prf-nhrgs/<your_name>/res/slurm-%j.out\n",
    "#SBATCH --mail-user=...  # Adjust this to match your email address.\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "module purge # Unload all models.\n",
    "module load vis/torchvision/0.13.1-foss-2022a-CUDA-11.7.0 # Load required modules.\n",
    "\n",
    "# Change 5-digit MASTER_PORT as you wish, SLURM will raise Error if duplicated with others.\n",
    "export MASTER_PORT=12340\n",
    "\n",
    "# Get the first node name as master address.\n",
    "master_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n",
    "export MASTER_ADDR=$master_addr\n",
    "echo \"MASTER_ADDR=\"$MASTER_ADDR\n",
    "\n",
    "export PYDIR=/scratch/hpc-prf-nhrgs/<your_name>/py # Set path to your python scripts.\n",
    "export RESDIR=/scratch/hpc-prf-nhrgs/<your_name>/res/job_${SLURM_JOB_ID} # Set path to save results for this job.\n",
    "mkdir ${RESDIR} # Create results dir.\n",
    "cd ${RESDIR} # Change to results dir.\n",
    "\n",
    "srun python -u ${PYDIR}/alex_parallel.py # Run python script in parallel using srun.\n",
    "# Each process executes exactly the same script!\n",
    "mv ../slurm-${SLURM_JOBID}.out ${RESDIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! \n",
    "You have successfully trained a distributed data-parallel deep neural network in `PyTorch`. To analyze your results visually, you can now plot the evolution of the loss, training accuracy, and validation accuracy over the training, e.g., with `matplotlib.pyplot`.\n",
    "### *Short break with everyone to discuss your results and possible solutions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
